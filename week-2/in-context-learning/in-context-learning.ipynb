{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HQg2WzITRaB"
      },
      "source": [
        "# In-Context Learning\n",
        "\n",
        "\n",
        "In-context learning is a generalisation of few-shot learning where the LLM is provided a context as part of the prompt and asked to respond by utilising the information in the context.\n",
        "\n",
        "* Example: *\"Summarize this research article into one paragraph highlighting its strengths and weaknesses: [insert article text]”*\n",
        "* Example: *\"Extract all the quotes from this text and organize them in alphabetical order: [insert text]”*\n",
        "\n",
        "A very popular technique that you will learn in week 5 called Retrieval-Augmented Generation (RAG) is a form of in-context learning, where:\n",
        "* a search engine is used to retrieve some relevant information\n",
        "* that information is then provided to the LLM as context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOdYNxNbTRaC"
      },
      "source": [
        "In this example we download some recent research papers from arXiv papers, extract the text from the PDF files and ask Gemini to summarize the articles as well as provide the main strengths and weaknesses of the papers. Finally we print the summaries to a local html file and as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tCaIcgsfTRaC",
        "outputId": "77ddd5dc-bd50-47d6-bd0f-75ae1209fc69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading pypdf-6.5.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, bs4\n",
            "Successfully installed bs4-0.0.2 pypdf-6.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests bs4 google-generativeai pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NR_m184bTRaD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "from urllib.request import urlopen, urlretrieve\n",
        "from IPython.display import Markdown, display\n",
        "from pypdf import PdfReader\n",
        "from datetime import date\n",
        "from tqdm import tqdm\n",
        "# from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NfDp-fY7TRaD"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"#\"\n",
        "genai.configure(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZcKTXcFTRaD"
      },
      "source": [
        "We select those papers that have been featured in Hugging Face papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9XDXe2lyTRaD"
      },
      "outputs": [],
      "source": [
        "BASE_URL = \"https://huggingface.co/papers\"\n",
        "page = requests.get(BASE_URL)\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "h3s = soup.find_all(\"h3\")\n",
        "\n",
        "papers = []\n",
        "\n",
        "for h3 in h3s:\n",
        "    a = h3.find(\"a\")\n",
        "    title = a.text\n",
        "    link = a[\"href\"].replace('/papers', '')\n",
        "\n",
        "    papers.append({\"title\": title, \"url\": f\"https://arxiv.org/pdf{link}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMRnRIJxTRaD"
      },
      "source": [
        "Code to extract text from PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RNkBqUY9TRaD"
      },
      "outputs": [],
      "source": [
        "def extract_paper(url):\n",
        "    html = urlopen(url).read()\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "    # kill all script and style elements\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()    # rip it out\n",
        "\n",
        "    # get text\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    # break multi-headlines into a line each\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    # drop blank lines\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_pdf(url):\n",
        "    pdf = urlretrieve(url, \"pdf_file.pdf\")\n",
        "    reader = PdfReader(\"pdf_file.pdf\")\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4B_XT5g0TRaE"
      },
      "outputs": [],
      "source": [
        "LLM = \"gemini-2.5-flash\"\n",
        "model = genai.GenerativeModel(LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK2dvJe-TRaE"
      },
      "source": [
        "We use Gemini to summarize the papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xn7VfngzTRaE",
        "outputId": "7fb1e8e4-5c59-433a-eb4e-08be2876b223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 4/7 [00:38<00:27,  9.09s/it]WARNING:pypdf.generic._base:could not convert string to float: b'0.00-37480314' : FloatObject (b'0.00-37480314') invalid; use 0.0 instead\n",
            "WARNING:pypdf.generic._base:could not convert string to float: b'0.00-37480314' : FloatObject (b'0.00-37480314') invalid; use 0.0 instead\n",
            "100%|██████████| 7/7 [01:07<00:00,  9.65s/it]\n"
          ]
        }
      ],
      "source": [
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "        paper[\"summary\"] = model.generate_content(\"Summarize this research article into one paragraph without formatting highlighting its strengths and weaknesses. \" + extract_pdf(paper[\"url\"])).text\n",
        "    except:\n",
        "        print(\"Generation failed\")\n",
        "        paper[\"summary\"] = \"Paper not available\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8NYJ8_rTRaE"
      },
      "source": [
        "We print the results to a html file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tcGbm_tRTRaE"
      },
      "outputs": [],
      "source": [
        "page = f\"<html> <head> <h1>Daily Dose of AI Research</h1> <h4>{date.today()}</h4> <p><i>Summaries generated with: {LLM}</i>\"\n",
        "with open(\"papers.html\", \"w\") as f:\n",
        "    f.write(page)\n",
        "for paper in papers:\n",
        "    page = f'<h2><a href=\"{paper[\"url\"]}\">{paper[\"title\"]}</a></h2> <p>{paper[\"summary\"]}</p>'\n",
        "    with open(\"papers.html\", \"a\") as f:\n",
        "        f.write(page)\n",
        "end = \"</head>  </html>\"\n",
        "with open(\"papers.html\", \"a\") as f:\n",
        "    f.write(end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvDtqUQnTRaE"
      },
      "source": [
        "We can also print the results to this notebook as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ggQ6L8YOTRaE",
        "outputId": "4146cc41-806b-4065-dca7-3e86abc337a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Latent Implicit Visual Reasoning](https://arxiv.org/pdf/2512.21218)**<br>This research introduces Latent Implicit Visual Reasoning (LIVR), a novel task-agnostic method designed to enhance Large Multimodal Models (LMMs) in predominantly visual reasoning tasks, addressing their inherent text-centric bias and the limitations of explicit visual supervision. LIVR achieves this by training LMMs to autonomously discover and utilize visual reasoning tokens through a two-stage visual bottlenecking approach, eliminating the need for costly annotations or hand-crafted intermediate visual steps. A key strength of LIVR is its consistent outperformance of direct supervised fine-tuning and achievement of state-of-the-art results across a diverse range of perception-heavy tasks, including those where defining intermediate visual abstractions is particularly challenging (like Art Style or Visual Similarity), while also demonstrating strong generalization capabilities in multi-task instruction tuning across multiple LMM backbones. The method's ability to implicitly learn useful visual structures without additional data or human biases is a significant advantage. However, a notable weakness of LIVR is that these latent tokens are inherently less interpretable compared to explicit textual explanations, making it more challenging to trace the model's internal reasoning. Furthermore, while effective, the method's performance can be sensitive to the number of latent tokens used, and the authors note future work on scaling to even larger models and datasets.<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning](https://arxiv.org/pdf/2512.20605)**<br>This research introduces \"internal RL,\" a novel hierarchical reinforcement learning method that overcomes the inefficiency of token-by-token exploration in autoregressive models by operating within their internal representations. A key strength is the demonstration that pretrained autoregressive models inherently learn behaviorally meaningful temporal abstractions in their internal activations, which an unsupervised metacontroller then learns to discover, sequence, and terminate without explicit labels. This \"internal RL\" approach significantly outperforms standard RL finetuning and existing hierarchical RL methods on sparse-reward grid world and MuJoCo tasks, enabling efficient exploration and compositional generalization through a learned, quasi-binary switching mechanism that dynamically compresses time. However, the study acknowledges the controlled nature of its experimental setup, noting that scalability to larger models and real-world tasks remains a future challenge. Additionally, optimal metacontroller performance relies on high-quality expert demonstrations for its self-supervised training, and the base model must be kept frozen during this phase, a constraint shown to be critical for the approach's success and the discovery of meaningful abstractions.<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Spatia: Video Generation with Updatable Spatial Memory](https://arxiv.org/pdf/2512.15716)**<br>Spatia introduces a novel spatial memory-aware framework for video generation, addressing the pervasive challenge of maintaining long-term spatial and temporal consistency in generated videos. It achieves this by explicitly preserving and iteratively updating a 3D scene point cloud as persistent spatial memory, leveraging visual SLAM to incorporate new content. This design enables several key strengths: robust dynamic-static disentanglement, allowing generation of dynamic entities within a coherent static scene; enhanced spatial consistency across multiple viewpoints; precise 3D-aware camera control; and intuitive interactive editing of scene elements. The model demonstrates superior visual quality and long-horizon consistency across various benchmarks, particularly excelling in closed-loop generation by effectively \"remembering\" revisited locations. However, the approach has notable weaknesses, including its heavy reliance on external 3D reconstruction (MapAnything) for initial scene estimation and memory updates, which introduces a dependency and potential points of failure or inaccuracy. Furthermore, it requires a separate segmentation step to remove dynamic entities before point cloud estimation, adding preprocessing complexity. While enhancing consistency, the iterative updating of the scene point cloud for \"all previously generated frames\" could pose scalability challenges for extremely long sequences, and maintaining high point cloud density for optimal visual quality consumes significant computational resources.<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Schoenfeld's Anatomy of Mathematical Reasoning by Language Models](https://arxiv.org/pdf/2512.19995)**<br>This research introduces ThinkARM, a novel framework that applies Schoenfeld's Episode Theory to systematically analyze the internal reasoning processes of large language models during mathematical problem-solving. The study's key strength lies in its ability to abstract complex LLM reasoning traces into explicit, functional steps like Analysis, Explore, Implement, and Verify, revealing a reproducible \"cognitive heartbeat\" in reasoning models that progresses from abstract conceptualization to concrete execution and finally to evaluative control. Through large-scale annotation using GPT-5, ThinkARM effectively distinguishes the structured, iterative reasoning of advanced models—characterized by balanced effort distribution and frequent Explore-Monitor/Verify loops—from the more feed-forward, execution-heavy patterns of non-reasoning models. Furthermore, it diagnostically links specific episode transition patterns to solution correctness, showing that successful solutions route exploratory uncertainty into monitoring or re-analysis, and highlights how efficiency-oriented methods selectively prune evaluative feedback steps rather than merely shortening responses. However, a notable weakness is the framework's reliance on an automatic LLM annotator, which, despite strong human agreement, could introduce labeling noise. Additionally, its primary focus on mathematical problem-solving limits the immediate generalizability of these findings to other diverse reasoning domains.<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[How Much 3D Do Video Foundation Models Encode?](https://arxiv.org/pdf/2512.19949)**<br>This research quantifies the inherent 3D understanding within Video Foundation Models (VidFMs) by developing a model-agnostic framework that probes frozen VidFM features to estimate 3D points, depth maps, and camera poses. Its primary strength lies in demonstrating that state-of-the-art video generative models, such as WAN and Open-Sora2.0, develop strong, generalizable 3D awareness—often surpassing specialized 3D models on out-of-domain data and significantly outperforming image-only models, underscoring the critical role of temporal reasoning. The study also offers valuable insights into optimal feature extraction layers and timesteps within diffusion models, and highlights the practical benefit of VidFM features for feedforward 3D reconstruction in limited-data regimes. However, a notable weakness is the reliance on publicly available checkpoints, which precluded controlled experimentation on factors like data scale or training strategy, thus limiting definitive attribution of observed differences, such as the mixed impact of model scaling and the potential for 3D fine-tuning to degrade generalization. Additionally, resource constraints prevented the training of large-scale 3D reconstruction models from scratch using VidFM features on massive datasets.<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation](https://arxiv.org/pdf/2512.19680)**<br>VA-π introduces a novel, lightweight post-training framework designed to address the critical misalignment in autoregressive (AR) visual generation where models optimized for token likelihood often produce low-quality images when decoded from tokenizers trained solely for pixel reconstruction. Its primary strength lies in formulating this problem as a variational optimization, deriving an Evidence Lower Bound (ELBO) that unifies pixel reconstruction with AR modeling by treating the generator as an RL policy and leveraging pixel-space reconstruction quality (derived from teacher forcing) as an intrinsic reward, alongside a regularization term for token distribution consistency. This approach is highly efficient and adaptable, achieving substantial improvements in image fidelity (FID, IS) and compositional understanding (GenEval) on models like LlamaGen and Janus-Pro with minimal data and compute (e.g., 1% ImageNet-1K, 25 minutes), without requiring tokenizer retraining or external reward models. However, while VA-π effectively bridges the token-pixel gap through intrinsic pixel-level guidance, its performance is inherently bounded by the capabilities of the fixed, pre-trained tokenizer, and although the regularization term aims to maintain distributional consistency, the reliance on teacher forcing for reward calculation does not fundamentally resolve the exposure bias problem that arises during free-running AR generation.<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/pdf/2512.13043)**<br>GTR-Turbo introduces an efficient approach to training multi-turn vision-language model agents by leveraging a \"free\" teacher derived from merging historical checkpoints generated during the ongoing reinforcement learning process, eliminating the need for costly and often inaccessible external models like GPT or Gemini. This innovative framework guides the agent's reasoning through either supervised fine-tuning or soft logit distillation, leading to significant strengths such as a 10-30% accuracy improvement over baselines, a 50% reduction in wall-clock training time, and a 60% decrease in compute cost, all while stabilizing training and mitigating \"thought collapse.\" However, the system's reliance on self-evolution means a foundational level of capability is required from the base model to prevent issues like passive exploration, potentially necessitating external knowledge for models with very low initial success rates. Additionally, the reported experimental results primarily focus on 7B models, leaving the framework's performance characteristics across larger model scales for future investigation.<br><br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for paper in papers:\n",
        "    printmd(\"**[{}]({})**<br>{}<br><br>\".format(paper[\"title\"],\n",
        "                                                paper[\"url\"],\n",
        "                                                paper[\"summary\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Version"
      ],
      "metadata": {
        "id": "Sy77JAIhUEjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Summarize this research article. The output must be a Markdown table \"\n",
        "    \"with exactly two columns: 'Strengths' and 'Weaknesses'. \"\n",
        "    \"Provide a concise summary of the key points in each column. \"\n",
        "    \"Do not include any other text or formatting outside the table. \"\n",
        ")\n",
        "\n",
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "        paper[\"summary\"] = model.generate_content(prompt + extract_pdf(paper[\"url\"])).text\n",
        "    except:\n",
        "        print(\"Generation failed\")\n",
        "        paper[\"summary\"] = \"Paper not available\""
      ],
      "metadata": {
        "id": "zmBplHYtUAKP",
        "outputId": "195f561b-e141-4e98-ae58-1ea35eab149a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 4/7 [00:51<00:34, 11.65s/it]WARNING:pypdf.generic._base:could not convert string to float: b'0.00-37480314' : FloatObject (b'0.00-37480314') invalid; use 0.0 instead\n",
            "WARNING:pypdf.generic._base:could not convert string to float: b'0.00-37480314' : FloatObject (b'0.00-37480314') invalid; use 0.0 instead\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.20ms\n",
            " 71%|███████▏  | 5/7 [00:52<00:15,  7.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation failed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 281.60ms\n",
            " 86%|████████▌ | 6/7 [00:53<00:05,  5.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation failed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:55<00:00,  4.48s/it]WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 281.95ms\n",
            "100%|██████████| 7/7 [00:55<00:00,  7.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation failed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page = f\"<html> <head> <h1>Daily Dose of AI Research</h1> <h4>{date.today()}</h4> <p><i>Summaries generated with: {LLM}</i>\"\n",
        "with open(\"papers.html\", \"w\") as f:\n",
        "    f.write(page)\n",
        "for paper in papers:\n",
        "    page = f'<h2><a href=\"{paper[\"url\"]}\">{paper[\"title\"]}</a></h2> <p>{paper[\"summary\"]}</p>'\n",
        "    with open(\"papers.html\", \"a\") as f:\n",
        "        f.write(page)\n",
        "end = \"</head>  </html>\"\n",
        "with open(\"papers.html\", \"a\") as f:\n",
        "    f.write(end)"
      ],
      "metadata": {
        "id": "IWQQW7PbUP-w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for paper in papers:\n",
        "    printmd(\"**[{}]({})**<br>{}<br><br>\".format(paper[\"title\"],\n",
        "                                                paper[\"url\"],\n",
        "                                                paper[\"summary\"]))"
      ],
      "metadata": {
        "id": "D5DKnjdIUfQy",
        "outputId": "8f17b82d-b0cd-409e-8be3-33b56c9fa1dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Latent Implicit Visual Reasoning](https://arxiv.org/pdf/2512.21218)**<br>| Strengths                                                                                                                                                                                                                          | Weaknesses                                                                                                                                                                                                                                                                                              |\n| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| - **Novelty & Efficiency:** Introduces Latent Implicit Visual Reasoning (LIVR) that implicitly learns visual representations via latent tokens and a visual bottleneck, requiring no explicit supervision, extra data, or annotation costs. | - **Interpretability:** Latent tokens are inherently less interpretable than explicit textual explanations or human-designed intermediate visual steps.                                                                                                                                                     |\n| - **Superior Performance:** Consistently outperforms direct supervised fine-tuning (SFT) and explicitly supervised latent reasoning methods (e.g., Mirage) across three diverse LMM backbones and nine perception-heavy tasks.            | - **Scalability (Current Scope):** While effective, the paper suggests future work on scaling to larger models and datasets, implying current evaluations are not at the absolute largest scales.                                                                                                    |\n| - **Strong Generalization:** Demonstrates robust performance and generalizes well in both single-task and multi-task instruction tuning settings.                                                                                          | - **Latent Capacity Optimization:** Performance can decrease with an excessive number of latent tokens (e.g., K=32), indicating a need for careful balancing or potentially task-specific tuning for optimal latent capacity.                                                                        |\n| - **Enhanced Visual Reasoning:** Particularly effective on complex visual abstraction tasks where human-defined intermediate steps are difficult, indicating deeper learned visual understanding.                                       | - **Hyperparameter Sensitivity:** The method's effectiveness is sensitive to architectural and training choices like latent token placement, specific masking strategy, and the Stage 1/Stage 2 training schedule, requiring careful configuration for peak performance. |\n| - **Validated Mechanism:** Ablation studies confirm that both dedicated latent tokens and the visual bottleneck are essential for LIVR's gains and that latents actively encode task-relevant visual information.                        |                                                                                                                                                                                                                                                                                                         |<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning](https://arxiv.org/pdf/2512.20605)**<br>| Strengths                                                                                                                                                                                                                                                                                                                                                                                                                    | Weaknesses                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Efficient Hierarchical Reinforcement Learning (HRL):** Overcomes the inefficiency of token-by-token exploration in autoregressive models by operating on temporally-abstract actions, enabling learning from sparse rewards where standard RL fails.                                                                                                                                                                                            | **Limited Experimental Scope:** The demonstrated \"overwhelming success\" is acknowledged to be within a \"controlled nature of our experimental setup\" (grid world and MuJoCo tasks), and its applicability to larger-scale or real-world environments, including LLMs, is explicitly stated as future work.                                                                                                                                      |\n| **Unsupervised Temporal Abstraction Discovery:** Introduces a novel metacontroller architecture that learns to discover and sequence meaningful temporally-abstract actions, along with their termination conditions, from unlabeled behavioral data without requiring explicit supervision.                                                                                                                                                          | **Reliance on Frozen Pretrained Model:** The method critically depends on pretraining a base autoregressive model and then freezing its parameters. Co-training the metacontroller with the base model, or training from scratch, leads to degenerate abstract action representations and failure in subsequent RL.                                                                                                                                 |\n| **Leverages Internal Representations:** Demonstrates that pretrained autoregressive models inherently form linearly controllable, temporally-abstract action representations in their internal residual stream activations, which can be effectively steered by external controllers.                                                                                                                                                             | **Sensitivity to Demonstration Quality:** Empirical evidence suggests that the metacontroller performs better when trained on \"cleaner\" (more optimal) expert demonstrations, implying potential sensitivity to the quality and optimality of the unlabeled behavioral dataset used for its self-supervised learning phase.                                                                                                                                   |\n| **Superior Performance on Challenging Tasks:** Achieves high success rates and efficient credit assignment on hierarchically-structured, sparse-reward tasks where standard RL finetuning (e.g., GRPO) and existing HRL methods (e.g., CompILE) fail to learn effectively.                                                                                                                                                                    | **Comparison Baselines:** While impressive, the performance gap is shown on tasks designed to be extremely difficult for token-level RL (e.g., \"minuscule chance\" of success by random sampling), which might inflate the relative advantage compared to more competitive scenarios or different task complexities.                                                                                                                                |\n| **Compositional Generalization:** The learned internal controllers can be sequenced to solve complex tasks requiring combinations of subgoals that were not explicitly seen during the metacontroller's training.                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| **Theoretically Grounded Architecture:** Provides a concrete neural architecture that aligns with Schmidhuber's theoretical framework of alternating self-supervised learning for history compression and reinforcement learning for control, demonstrating empirical backing for these ideas. |                                                                                                                                                                                                                                                                                                                                                                                                                                            |<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Spatia: Video Generation with Updatable Spatial Memory](https://arxiv.org/pdf/2512.15716)**<br>| Strengths                                                                                             | Weaknesses                                                                                               |\n| :--------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------- |\n| **Persistent 3D Spatial Memory:** Novel use of an updatable 3D point cloud ensures long-term spatial and temporal consistency in generated videos. | **Reliance on External Models:** Integrates several off-the-shelf 3D reconstruction/SLAM and video diffusion models, making it dependent on their performance and limitations. |\n| **Dynamic-Static Disentanglement:** Enables generating dynamic elements while maintaining a consistent static scene, surpassing static-only generation limits. | **Resource Intensiveness:** Training and maintaining dense 3D spatial memory requires substantial computational resources (e.g., 64 GPUs for training), and point cloud density affects quality. |\n| **Explicit 3D Control:** Offers precise camera control via 3D trajectories and supports interactive, 3D-aware editing of scene elements. | **Static-Only Memory:** The explicit spatial memory mechanism models only the static scene, potentially limiting complex dynamic interactions that fundamentally alter the scene geometry within the memory itself. |\n| **High Performance & Long-Horizon Coherence:** Achieves superior visual quality and robust spatial coherence over extended video durations, validated across benchmarks. | **Sensitivity to Point Cloud Density:** Achieving fine-grained spatial guidance requires high point cloud density, which directly impacts memory storage and potentially increases processing time, as shown in ablation studies. |<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Schoenfeld's Anatomy of Mathematical Reasoning by Language Models](https://arxiv.org/pdf/2512.19995)**<br>| Strengths                                                                                                                                                                                                                                                         | Weaknesses                                                                                                                                                                                             |\n| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Introduces ThinkARM, a scalable framework for abstracting LLM reasoning into explicit functional steps (e.g., Analysis, Explore, Implement, Verify) using Schoenfeld’s Episode Theory.                  | Relies on an automatic annotator (GPT-5), which, despite high agreement with human annotations, may introduce labeling noise into the large-scale dataset.                                            |\n| Reveals reproducible thinking dynamics, distinct lexical patterns for each reasoning step, and a three-phase \"cognitive heartbeat\" (Initialization, Execution, Convergence) across LLMs.               | The primary focus of the experiments is on mathematical problem-solving, limiting the immediate generalizability of findings to other domains and complex reasoning tasks.                         |\n| Differentiates reasoning from non-reasoning models by structural effort distribution (e.g., balanced vs. Implement-heavy) and the presence of iterative feedback loops (e.g., Explore-Monitor/Verify). |                                                                                                                                                                                                        |\n| Provides diagnostic insights by linking episode-level patterns to solution correctness (e.g., routing exploration to monitoring/analysis for correct solutions) and analyzing efficiency methods.     |                                                                                                                                                                                                        |<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[How Much 3D Do Video Foundation Models Encode?](https://arxiv.org/pdf/2512.19949)**<br>Paper not available<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation](https://arxiv.org/pdf/2512.19680)**<br>Paper not available<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/pdf/2512.13043)**<br>Paper not available<br><br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9RL_UOdUhcY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}